import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection import KFold
import random
from tensorflow.keras import models, layers  # Import models and layers
from scipy.ndimage import rotate
from skimage.transform import resize
import scipy.ndimage
from tensorflow.keras.callbacks import Callback
from sklearn.model_selection import train_test_split

# Load MNIST data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Normalize the data to range [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Add channel dimension for CNN
x_train = x_train[..., np.newaxis]  # Shape (60000, 28, 28, 1)
x_test = x_test[..., np.newaxis]    # Shape (10000, 28, 28, 1)

# One-hot encode the labels
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# CNN model
def cnn_model(input_shape):
    model = models.Sequential([
        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Cross-Validation (This Function was taken out of the Paper)
def cross_validate_cnn(x, y, n_splits=5, epochs=5, batch_size=64):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    fold_accuracies = []

    for train_index, val_index in kf.split(x):
        x_fold_train, x_fold_val = x[train_index], x[val_index]
        y_fold_train, y_fold_val = y[train_index], y[val_index]

        # Build and train the model
        model = cnn_model(input_shape=(28, 28, 1))
        model.fit(x_fold_train, y_fold_train, epochs=epochs, batch_size=batch_size, verbose=0)

        # Evaluate on validation set
        val_loss, val_accuracy = model.evaluate(x_fold_val, y_fold_val, verbose=0)
        fold_accuracies.append(val_accuracy)

    return np.mean(fold_accuracies), np.std(fold_accuracies)

# Accuracy vs Training Set Size
def plot_learning_curve(x, y, fractions, n_splits=5, epochs=5, batch_size=64):
    mean_accuracies = []
    std_accuracies = []

    for fraction in fractions:
        # Subset the data
        num_samples = int(len(x) * fraction)
        indices = random.sample(range(len(x)), num_samples)
        x_subset, y_subset = x[indices], y[indices]

        # Perform cross-validation on the subset
        mean_acc, std_acc = cross_validate_cnn(x_subset, y_subset, n_splits=n_splits, epochs=epochs, batch_size=batch_size)
        mean_accuracies.append(mean_acc)
        std_accuracies.append(std_acc)

    # Plot Accuracy vs Training Set Size
    plt.figure(figsize=(10, 6))
    plt.errorbar(fractions, mean_accuracies, yerr=std_accuracies, fmt='-o', capsize=5)
    plt.title('Accuracy vs Training Set Size')
    plt.xlabel('Fraction of Training Set Used')
    plt.ylabel('Cross-Validated Accuracy')
    plt.grid(True)
    plt.show()

random.seed(2424)

#Perform cross-validation on the Original
mean_acc, std_acc = cross_validate_cnn(x_train, y_train, n_splits=5, epochs=5, batch_size=64)

# Evaluate test set
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=0)
test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)

# Print results
print(f"Cross-validated Accuracy: {mean_acc:.4f} ± {std_acc:.4f}")
print(f"Test Set Accuracy: {test_accuracy:.4f}")
print(f"Test Set Error Rate: {1 - test_accuracy:.4f}")

# Plot the Curve
fractions = [1.0, 0.75, 0.5, 0.25, 0.1, 0.05]
fractions_1 = [1.0, 0.75, 0.5, 0.25, 0.1, 0.05, 0.01]
plot_learning_curve(x_train, y_train, fractions, n_splits=5, epochs=5, batch_size=64)

plot_learning_curve(x_train, y_train, fractions_1, n_splits=5, epochs=5, batch_size=64)

random.seed(2424)
# Randomly select 5% of the MNIST training data
fraction = 0.05
num_samples = int(len(x_train) * fraction)
indices = random.sample(range(len(x_train)), num_samples)
x_subset, y_subset = x_train[indices], y_train[indices]

# Define the rotation angles
rotation_angles_list1 = [angle for angle in range(-45, 50, 5) if angle != 0]  # -45 to 45 degrees in steps of 5
rotation_angles_list2 = [angle for angle in range(-90, 95, 5) if angle != 0]  # -90 to 90 degrees in steps of 5
rotation_angles_list3 = [angle for angle in range(-180, 185, 5) if angle != 0]  # -180 to 90 degrees in steps of 5

padding_size = 8  # Padding to prevent digit cutoff

# Function to pad, rotate, and resize images
def pad_rotate_resize_images(images, angles, target_size=(28, 28)):
    """
    Pads, rotates, and resizes the given images.

    Parameters:
    - images (np.ndarray): Array of images to be processed (n_samples, 28, 28, 1).
    - angles (list of int): List of angles to rotate the images.
    - target_size (tuple of int): Final size of the images after resizing (height, width).

    Returns:
    - augmented_images (np.ndarray): Rotated and resized images.
    """
    augmented_images = []

    for image in images:
        # Pad the image
        padded_image = np.pad(image, ((padding_size, padding_size), (padding_size, padding_size), (0, 0)), mode='constant', constant_values=0)

        # Rotate and resize the padded image for each angle
        for angle in angles:
            rotated_image = rotate(padded_image, angle, reshape=False, mode='constant', cval=0)
            resized_image = resize(rotated_image, target_size, mode='constant', anti_aliasing=True)
            augmented_images.append(resized_image)

    return np.array(augmented_images)

# Process the data for rotation_angles_list1
x_augmented_list1 = pad_rotate_resize_images(x_subset, rotation_angles_list1)
y_augmented_list1 = np.repeat(y_subset, len(rotation_angles_list1), axis=0)

# Process the data for rotation_angles_list2
x_augmented_list2 = pad_rotate_resize_images(x_subset, rotation_angles_list2)
y_augmented_list2 = np.repeat(y_subset, len(rotation_angles_list2), axis=0)

# Process the data for rotation_angles_list3
x_augmented_list3 = pad_rotate_resize_images(x_subset, rotation_angles_list3)
y_augmented_list3 = np.repeat(y_subset, len(rotation_angles_list3), axis=0)

# Combine data for each case
x_combined_list1 = np.concatenate((x_subset, x_augmented_list1), axis=0)
y_combined_list1 = np.concatenate((y_subset, y_augmented_list1), axis=0)

x_combined_list2 = np.concatenate((x_subset, x_augmented_list2), axis=0)
y_combined_list2 = np.concatenate((y_subset, y_augmented_list2), axis=0)

x_combined_list3 = np.concatenate((x_subset, x_augmented_list3), axis=0)
y_combined_list3 = np.concatenate((y_subset, y_augmented_list3), axis=0)

# Print results
print(f"Original 5% subset size: {x_subset.shape[0]}")
print(f"Augmented subset size (including original, from -45 to 45 degrees): {x_combined_list1.shape[0]}")
print(f"Augmented subset size (including original, from -90 to 90 degrees): {x_combined_list2.shape[0]}")
print(f"Augmented subset size (including original, from -180 to 180 degrees): {x_combined_list3.shape[0]}")

# Cross-validation on the original 5% subset
mean_acc_original, std_acc_original = cross_validate_cnn(x_subset, y_subset, n_splits=5, epochs=5, batch_size=64)

# Evaluate on the original 5% subset
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_subset, y_subset, epochs=5, batch_size=64, verbose=0)
test_acc_original, test_acc_original = model.evaluate(x_test, y_test, verbose=0)

# Cross-validation on the augmented dataset (including original, from -45 to 45 degrees)
mean_acc_augmented1, std_acc_augmented1 = cross_validate_cnn(x_combined_list1, y_combined_list1, n_splits=5, epochs=5, batch_size=64)

# Evaluate on the original 5% subset (including original, from -45 to 45 degrees)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_combined_list1, y_combined_list1, epochs=5, batch_size=64, verbose=0)
test_acc_original1, test_acc_original1 = model.evaluate(x_test, y_test, verbose=0)

# Cross-validation on the augmented dataset (including original, from -90 to 90 degrees)
mean_acc_augmented2, std_acc_augmented2 = cross_validate_cnn(x_combined_list2, y_combined_list2, n_splits=5, epochs=5, batch_size=64)

# Evaluate on the original 5% subset (including original, from -90 to 90 degrees)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_combined_list2, y_combined_list2, epochs=5, batch_size=64, verbose=0)
test_acc_original2, test_acc_original2 = model.evaluate(x_test, y_test, verbose=0)

# Cross-validation on the augmented dataset (including original, from -180 to 180 degrees)
mean_acc_augmented3, std_acc_augmented3 = cross_validate_cnn(x_combined_list3, y_combined_list3, n_splits=5, epochs=5, batch_size=64)

# Evaluate on the original 5% subset (including original, from -180 to 180 degrees)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_combined_list2, y_combined_list2, epochs=5, batch_size=64, verbose=0)
test_acc_original3, test_acc_original3 = model.evaluate(x_test, y_test, verbose=0)

# Print results for comparison
print(f"Cross-validated Accuracy on Original 5% Subset: {mean_acc_original:.4f} ± {std_acc_original:.4f}")
print(f"Test Set Accuracy: {test_acc_original:.4f}")
print(f"Cross-validated Accuracy on Augmented Data (including original, from -45 to 45 degrees): {mean_acc_augmented1:.4f} ± {std_acc_augmented1:.4f}")
print(f"Test Set Accuracy (including 5%, from -45 to 45 degrees): {test_acc_original1:.4f}")
print(f"Cross-validated Accuracy on Augmented Data (including original, from -90 to 90 degrees): {mean_acc_augmented2:.4f} ± {std_acc_augmented2:.4f}")
print(f"Test Set Accuracy (including 5%, from -90 to 90 degrees): {test_acc_original2:.4f}")
print(f"Cross-validated Accuracy on Augmented Data (including original, from -180 to 180 degrees): {mean_acc_augmented3:.4f} ± {std_acc_augmented3:.4f}")
print(f"Test Set Accuracy (including 5%, from -180 to 180 degrees): {test_acc_original3:.4f}")

random.seed(2424)
# Randomly select 1% of the MNIST training data
fraction = 0.01
num_samples = int(len(x_train) * fraction)
indices = random.sample(range(len(x_train)), num_samples)
x_subset_percent1, y_subset_percent1 = x_train[indices], y_train[indices]

# Process the data for rotation_angles_list1
x_augmented_list1_1 = pad_rotate_resize_images(x_subset_percent1, rotation_angles_list1)
y_augmented_list1_1 = np.repeat(y_subset_percent1, len(rotation_angles_list1), axis=0)

# Process the data for rotation_angles_list2
x_augmented_list2_1 = pad_rotate_resize_images(x_subset_percent1, rotation_angles_list2)
y_augmented_list2_1 = np.repeat(y_subset_percent1, len(rotation_angles_list2), axis=0)

# Process the data for rotation_angles_list3
x_augmented_list3_1 = pad_rotate_resize_images(x_subset_percent1, rotation_angles_list3)
y_augmented_list3_1 = np.repeat(y_subset_percent1, len(rotation_angles_list3), axis=0)

# Combine data for each case
x_combined_list1_1 = np.concatenate((x_subset_percent1, x_augmented_list1_1), axis=0)
y_combined_list1_1 = np.concatenate((y_subset_percent1, y_augmented_list1_1), axis=0)

x_combined_list2_1 = np.concatenate((x_subset_percent1, x_augmented_list2_1), axis=0)
y_combined_list2_1 = np.concatenate((y_subset_percent1, y_augmented_list2_1), axis=0)

x_combined_list3_1 = np.concatenate((x_subset_percent1, x_augmented_list3_1), axis=0)
y_combined_list3_1 = np.concatenate((y_subset_percent1, y_augmented_list3_1), axis=0)

# Print results
print(f"Original 1% subset size: {x_subset_percent1.shape[0]}")
print(f"Augmented subset size (including original, from -45 to 45 degrees): {x_combined_list1_1.shape[0]}")
print(f"Augmented subset size (including original, from -90 to 90 degrees): {x_combined_list2_1.shape[0]}")
print(f"Augmented subset size (including original, from -180 to 180 degrees): {x_combined_list3_1.shape[0]}")

# Cross-validation on the original 1% subset
mean_acc_original1, std_acc_original1 = cross_validate_cnn(x_subset_percent1, y_subset_percent1, n_splits=5, epochs=5, batch_size=64)

# Evaluate on the original 1% subset
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_subset_percent1, y_subset_percent1, epochs=5, batch_size=64, verbose=0)
test_acc_original1, test_acc_original1 = model.evaluate(x_test, y_test, verbose=0)

# Cross-validation on the augmented dataset (including original, from -45 to 45 degrees)
mean_acc_augmented1, std_acc_augmented1 = cross_validate_cnn(x_combined_list1_1 , y_combined_list1_1 , n_splits=5, epochs=5, batch_size=64)

# Evaluate on the original 1% subset (including original, from -45 to 45 degrees)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_combined_list1_1, y_combined_list1_1, epochs=5, batch_size=64, verbose=0)
test_acc_original1_1, test_acc_original1_1 = model.evaluate(x_test, y_test, verbose=0)

# Cross-validation on the augmented dataset (including original, from -90 to 90 degrees)
mean_acc_augmented2, std_acc_augmented2 = cross_validate_cnn(x_combined_list2_1 , y_combined_list2_1, n_splits=5, epochs=5, batch_size=64)

# Evaluate on the original 1% subset (including original, from -90 to 90 degrees)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_combined_list2_1, y_combined_list2_1, epochs=5, batch_size=64, verbose=0)
test_acc_original2_1, test_acc_original2_1 = model.evaluate(x_test, y_test, verbose=0)

# Cross-validation on the augmented dataset (including original, from -180 to 180 degrees)
mean_acc_augmented3, std_acc_augmented3 = cross_validate_cnn(x_combined_list3_1 , y_combined_list3_1, n_splits=5, epochs=5, batch_size=64)

# Evaluate on the original 1% subset (including original, from -90 to 90 degrees)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_combined_list3_1, y_combined_list3_1, epochs=5, batch_size=64, verbose=0)
test_acc_original3_1, test_acc_original3_1 = model.evaluate(x_test, y_test, verbose=0)

# Print results for comparison
print(f"Cross-validated Accuracy on Original 1% Subset: {mean_acc_original1:.4f} ± {std_acc_original1:.4f}")
print(f"Test Set Accuracy: {test_acc_original1:.4f}")
print(f"Cross-validated Accuracy on Augmented Data (including original, from -45 to 45 degrees): {mean_acc_augmented1:.4f} ± {std_acc_augmented1:.4f}")
print(f"Test Set Accuracy (including 1%, from -45 to 45 degrees): {test_acc_original1_1:.4f}")
print(f"Cross-validated Accuracy on Augmented Data (including original, from -90 to 90 degrees): {mean_acc_augmented2:.4f} ± {std_acc_augmented2:.4f}")
print(f"Test Set Accuracy (including 1%, from -90 to 90 degrees): {test_acc_original2_1:.4f}")
print(f"Cross-validated Accuracy on Augmented Data (including original, from -180 to 180 degrees): {mean_acc_augmented3:.4f} ± {std_acc_augmented3:.4f}")
print(f"Test Set Accuracy (including 1%, from -180 to 180 degrees): {test_acc_original3_1:.4f}")

# Elastic deformation function
def elastic_deformation(image, alpha, sigma, random_state=None):
    if random_state is None:
        random_state = np.random.RandomState(None)

    shape = image.shape
    dx = random_state.rand(*shape) * 2 - 1
    dy = random_state.rand(*shape) * 2 - 1

    dx = scipy.ndimage.gaussian_filter(dx, sigma) * alpha
    dy = scipy.ndimage.gaussian_filter(dy, sigma) * alpha

    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))
    indices = (y + dy).reshape(-1), (x + dx).reshape(-1)

    deformed_image = scipy.ndimage.map_coordinates(image, indices, order=1, mode='reflect')
    return deformed_image.reshape(image.shape) #, dx, dy  # Ensure the shape matches the input and return displacement fields. dx and dy are used for visualization only. 

# Apply elastic deformation to a subset
def augment_subset_with_deformations(x_data, y_data, alpha, sigma):
    augmented_images = []
    augmented_labels = []
    random_state = np.random.RandomState(42)

    for image, label in zip(x_data, y_data):
        original_image = image[:, :, 0]  # Remove channel dimension
        deformed_image = elastic_deformation(original_image, alpha, sigma, random_state)
        augmented_images.append(deformed_image[..., np.newaxis])  # Add channel back
        augmented_labels.append(label)

    return np.array(augmented_images), np.array(augmented_labels)

# Parameters for elastic deformation 1
alpha = 1.4
sigma = 4

alpha2 = 1.7
sigma2 = 4

# Parameters for elastic deformation 3
alpha3 = 1.8
sigma3 = 4

# Visualize elastic deformation on a random image
random_index = np.random.randint(0, len(x_train))
original_image = x_train[random_index, :, :, 0]  # Remove channel dimension for processing

# Apply deformation
deformed_image, dx, dy = elastic_deformation(original_image, alpha, sigma)

# Warp original image using displacement fields
x, y = np.meshgrid(np.arange(original_image.shape[1]), np.arange(original_image.shape[0]))
warped_x = x + dx
warped_y = y + dy

# Plot original, deformed, displacement fields, and warped image
plt.figure(figsize=(16, 4))


# Original image
plt.subplot(1, 4, 1)
plt.imshow(original_image, cmap='gray')
plt.title('Original Image')
plt.axis('off')

# Deformed image
plt.subplot(1, 4, 2)
plt.imshow(deformed_image, cmap='gray')
plt.title('Deformed Image')
plt.axis('off')

# Original displacement fields
#plt.subplot(1, 4, 3)
#magnitude_original = np.sqrt((dx * 0)**2 + (dy * 0)**2)  # Zero displacement fields for original image
#plt.imshow(magnitude_original, cmap='viridis')
#plt.title('Original Displacement')
#plt.colorbar(label='Displacement')
#plt.axis('off')

# Displacement fields
#plt.subplot(1, 4, 4)
#magnitude = np.sqrt(dx**2 + dy**2)
#plt.imshow(magnitude, cmap='viridis')
#plt.title('Deformed Displacement Magnitude')
#plt.colorbar(label='Displacement')
#plt.axis('off')


# Warped image
plt.subplot(1, 4, 3)
plt.imshow(deformed_image, cmap='gray', alpha=0.5)
plt.quiver(x, y, dx, dy, color='red', angles='xy', scale_units='xy', scale=1)
plt.title('Warped Image with Displacement Vectors')
plt.axis('off')

plt.tight_layout()
plt.show()

random.seed(2424)
# Augment 1% subset
x_aug_1, y_aug_1 = augment_subset_with_deformations(x_subset_percent1, y_subset_percent1, alpha, sigma)
x_subset_percent1_combined = np.concatenate([x_subset_percent1, x_aug_1])
y_subset_percent1_combined = np.concatenate([y_subset_percent1, y_aug_1])

# Augment 5% subset
x_aug_5, y_aug_5 = augment_subset_with_deformations(x_subset, y_subset, alpha, sigma)
x_subset_percent5_combined = np.concatenate([x_subset, x_aug_5])
y_subset_percent5_combined = np.concatenate([y_subset, y_aug_5])

random.seed(2424)
# Augment 1% subset 2ed
x_aug_1_2, y_aug_1_2 = augment_subset_with_deformations(x_subset_percent1, y_subset_percent1, alpha2, sigma2)
x_subset_percent1_combined_2 = np.concatenate([x_subset_percent1_combined, x_aug_1_2])
y_subset_percent1_combined_2 = np.concatenate([y_subset_percent1_combined, y_aug_1_2])

# Augment 5% subset 2ed
x_aug_5_2, y_aug_5_2 = augment_subset_with_deformations(x_subset, y_subset,  alpha2, sigma2)
x_subset_percent5_combined_2 = np.concatenate([x_subset_percent5_combined, x_aug_5_2])
y_subset_percent5_combined_2 = np.concatenate([y_subset_percent5_combined, y_aug_5_2])

random.seed(2424)
# Augment 1% subset 3ed
x_aug_1_3, y_aug_1_3 = augment_subset_with_deformations(x_subset_percent1, y_subset_percent1, alpha3, sigma3)
x_subset_percent1_combined_3 = np.concatenate([x_subset_percent1_combined_2, x_aug_1_3])
y_subset_percent1_combined_3 = np.concatenate([y_subset_percent1_combined_2, y_aug_1_3])

# Augment 5% subset 3ed
x_aug_5_3, y_aug_5_3 = augment_subset_with_deformations(x_subset, y_subset,  alpha3, sigma3)
x_subset_percent5_combined_3 = np.concatenate([x_subset_percent5_combined_2, x_aug_5_3])
y_subset_percent5_combined_3 = np.concatenate([y_subset_percent5_combined_2, y_aug_5_3])

print(f"Augmented subset size (including original, from -45 to 45 degrees): {x_subset_percent1_combined_3.shape[0]}")
print(f"Augmented subset size (including original, from -90 to 90 degrees): {y_subset_percent5_combined_3.shape[0]}")

# Cross-validation on the augmented 5% subset (1 ed)
mean_acc_augmented5, std_acc_augmented5 = cross_validate_cnn(x_subset_percent5_combined, y_subset_percent5_combined)
print(f"5% augmented subset - Mean accuracy: {mean_acc_augmented5:.4f}, Std: {std_acc_augmented5:.4f}")

# Train and evaluate on the augmented 5% subset (1 ed)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_subset_percent5_combined, y_subset_percent5_combined, epochs=5, batch_size=64, verbose=1)
test_loss_augmented5, test_acc_augmented5 = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy on augmented 5% subset: {test_acc_augmented5:.4f}")

# Cross-validation on the augmented 5% subset (2 ed)
mean_acc_augmented5, std_acc_augmented5 = cross_validate_cnn(x_subset_percent5_combined_2, y_subset_percent5_combined_2)
print(f"5% augmented subset - Mean accuracy: {mean_acc_augmented5:.4f}, Std: {std_acc_augmented5:.4f}")

# Train and evaluate on the augmented 5% subset (2 ed)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_subset_percent5_combined_2, y_subset_percent5_combined_2, epochs=5, batch_size=64, verbose=1)
test_loss_augmented5, test_acc_augmented5 = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy on augmented 5% subset: {test_acc_augmented5:.4f}")

# Cross-validation on the augmented 5% subset (3 ed)
mean_acc_augmented5, std_acc_augmented5 = cross_validate_cnn(x_subset_percent5_combined_3, y_subset_percent5_combined_3)
print(f"5% augmented subset - Mean accuracy: {mean_acc_augmented5:.4f}, Std: {std_acc_augmented5:.4f}")

# Train and evaluate on the augmented 5% subset (3 ed)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_subset_percent5_combined_3, y_subset_percent5_combined_3, epochs=5, batch_size=64, verbose=1)
test_loss_augmented5, test_acc_augmented5 = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy on augmented 5% subset: {test_acc_augmented5:.4f}")

# Cross-validation on the augmented 1% subset (1 ed)
mean_acc_augmented1, std_acc_augmented1 = cross_validate_cnn(x_subset_percent1_combined, y_subset_percent1_combined)
print(f"1% augmented subset - Mean accuracy: {mean_acc_augmented1:.4f}, Std: {std_acc_augmented1:.4f}")

# Train and evaluate on the augmented 1% subset (1 ed)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_subset_percent1_combined, y_subset_percent1_combined, epochs=5, batch_size=64, verbose=1)
test_loss_augmented1, test_acc_augmented1 = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy on augmented 1% subset: {test_acc_augmented1:.4f}")

# Cross-validation on the augmented 1% subset (2 ed)
mean_acc_augmented1, std_acc_augmented1 = cross_validate_cnn(x_subset_percent1_combined_2, y_subset_percent1_combined_2)
print(f"1% augmented subset - Mean accuracy: {mean_acc_augmented1:.4f}, Std: {std_acc_augmented1:.4f}")

# Train and evaluate on the augmented 1% subset (2 ed)
model = cnn_model(input_shape=(28, 28, 1))
model.fit(x_subset_percent1_combined_2, y_subset_percent1_combined_2, epochs=5, batch_size=64, verbose=1)
test_loss_augmented1, test_acc_augmented1 = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy on augmented 1% subset: {test_acc_augmented1:.4f}")

print(f"Augmented subset size (including original, from -45 to 45 degrees): {x_subset_percent5_combined.shape[0]}")
print(f"Augmented subset size (including original, from -90 to 90 degrees): {x_subset_percent1_combined.shape[0]}")

# Evaluate on the original model
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss vs Epochs of Original MNIST Dataset')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on the original 5% subset
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_subset, y_subset, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss vs Epochs 5% of Original MNIST Dataset')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on the original 5% subset with 45 range
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_combined_list1, y_combined_list1, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('45 Degree Range Train Loss Compare to 5% of Original MNIST Dataset Validation Lose')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on the original 5% subset with 90 range
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_combined_list2, y_combined_list2, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('90 Degree Range Train Loss Compare to 5% of Original MNIST Dataset Validation Lose')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on the original 5% subset with 180 range
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_combined_list2, y_combined_list2, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('180 Degree Range Train Loss Compare to 5% of Original MNIST Dataset Validation Lose')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on the original 1% subset
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_subset_percent1, y_subset_percent1, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss vs Epochs 1% of Original MNIST Dataset')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on the original 1% subset
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_combined_list1_1, y_combined_list1_1, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('45 Degree Range Train Loss Compare to 1% of Original MNIST Dataset Validation Lose')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on the original 1% subset
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_combined_list2_1, y_combined_list2_1, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('90 Degree Range Train Loss Compare to 1% of Original MNIST Dataset Validation Lose')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on the original 1% subset
model = cnn_model(input_shape=(28, 28, 1))
history = model.fit(x_combined_list3_1, y_combined_list3_1, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))


# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('180 Degree Range Train Loss Compare to 1% of Original MNIST Dataset Validation Lose')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Train and evaluate on the augmented 1% subset
model = cnn_model(input_shape=(28, 28, 1))
history_augmented1 = model.fit(x_subset_percent1_combined, y_subset_percent1_combined, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))

# Plot training and validation loss for 1% augmented subset
plt.figure(figsize=(8, 6))
plt.plot(history_augmented1.history['loss'], label='Training Loss')
plt.plot(history_augmented1.history['val_loss'], label='Validation Loss')
plt.title('Loss vs Epochs - 1% Augmented Subset One Elastic Deformation ')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Train and evaluate on the augmented 1% subset
model = cnn_model(input_shape=(28, 28, 1))
history_augmented1 = model.fit(x_subset_percent1_combined_2, y_subset_percent1_combined_2, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))

# Plot training and validation loss for 1% augmented subset
plt.figure(figsize=(8, 6))
plt.plot(history_augmented1.history['loss'], label='Training Loss')
plt.plot(history_augmented1.history['val_loss'], label='Validation Loss')
plt.title('Loss vs Epochs - 1% Augmented Subset Two Elastic Deformation')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Train and evaluate on the augmented 5% subset
model = cnn_model(input_shape=(28, 28, 1))
history_augmented5 = model.fit(x_subset_percent5_combined, y_subset_percent5_combined, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))

# Plot training and validation loss for 5% augmented subset
plt.figure(figsize=(8, 6))
plt.plot(history_augmented5.history['loss'], label='Training Loss')
plt.plot(history_augmented5.history['val_loss'], label='Validation Loss')
plt.title('Loss vs Epochs - 5% Augmented Subset One Elastic Deformation')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Train and evaluate on the augmented 5% subset
model = cnn_model(input_shape=(28, 28, 1))
history_augmented5 = model.fit(x_subset_percent5_combined_2, y_subset_percent5_combined_2, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test))

# Plot training and validation loss for 5% augmented subset
plt.figure(figsize=(8, 6))
plt.plot(history_augmented5.history['loss'], label='Training Loss')
plt.plot(history_augmented5.history['val_loss'], label='Validation Loss')
plt.title('Loss vs Epochs - 5% Augmented Subset Two Elastic Deformation')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()
